{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /Users/eugenio/Library/Python/3.8/lib/python/site-packages (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in /Users/eugenio/Library/Python/3.8/lib/python/site-packages (from pyspark) (0.10.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch SOM in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our_som2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, Row, SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "\n",
    "class SOM:\n",
    "    \"\"\"\n",
    "    Implementation of Batch SOM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, net_x_dim, net_y_dim, num_features):\n",
    "        self.network_dimensions = np.array([net_x_dim, net_y_dim])\n",
    "        self.init_radius = min(self.network_dimensions[0], self.network_dimensions[1])\n",
    "        # initialize weight vectors\n",
    "        self.num_features = num_features\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.net = np.random.random((self.network_dimensions[0], self.network_dimensions[1], self.num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function for finding the distance and BMU (Eq (1) & (2)) is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def find_bmu(row_t, net):\n",
    "        \"\"\"\n",
    "            Find the best matching unit for a given vector, row_t, in the SOM\n",
    "            Returns: a (bmu, bmu_idx) tuple where bmu is the high-dimensional Best Matching Unit\n",
    "                     and bmu_idx is the index of this vector in the SOM\n",
    "        \"\"\"\n",
    "        net_size = np.shape(net)\n",
    "        x_size = net_size[0]\n",
    "        y_size = net_size[1]\n",
    "        num_features = net_size[2]\n",
    "        bmu_idx = np.array([0, 0])\n",
    "        # set the initial minimum distance to a huge number\n",
    "        min_dist = np.iinfo(np.int).max\n",
    "        # calculate the high-dimensional distance between each neuron and the input\n",
    "        # for (k = 1,..., K)\n",
    "        for x in range(x_size):\n",
    "            for y in range(y_size):\n",
    "                weight_k = net[x, y, :].reshape(1, num_features)\n",
    "                # compute distances dk using Eq. (2)\n",
    "                sq_dist = np.sum((weight_k - row_t) ** 2)\n",
    "                # compute winning node c using Eq. (3)\n",
    "                if sq_dist < min_dist:\n",
    "                    min_dist = sq_dist\n",
    "                    bmu_idx = np.array([x, y])\n",
    "        # get vector corresponding to bmu_idx\n",
    "        bmu = net[bmu_idx[0], bmu_idx[1], :].reshape(1, num_features)\n",
    "        return bmu, bmu_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the implementation of neighborhood function (Eq (4)) is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, df, num_epochs, resetWeights=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param df: input dataframe for training with \"features\" column\n",
    "        :param num_epochs: for how many epochs should the dataframe be trained\n",
    "        :param resetWeights: should the weights be randomized for next training\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if resetWeights:\n",
    "            self.initialize()\n",
    "        # Making sure the radius decays from initial value comparable to the dimension of network to 1\n",
    "        self.time_constant = num_epochs / np.log(self.init_radius)\n",
    "        # visualization\n",
    "        if self.num_features == 3:\n",
    "            fig = plt.figure()\n",
    "        else:\n",
    "            fig = None\n",
    "        # using spark's cache feature as the algorithm is iterative\n",
    "        rdd = df.rdd.cache()\n",
    "        # getting sparkContext\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        # for (epoch = 1,..., Nepochs)\n",
    "        for i in range(1, num_epochs + 1):\n",
    "            # interpolate new value for  \\sigma(t)\n",
    "            radius = self.decay_radius(i)\n",
    "            # visualization\n",
    "            vis_interval = int(num_epochs/10)\n",
    "            if i % vis_interval == 0:\n",
    "                if fig is not None:\n",
    "                    self.show_plot(fig, i/vis_interval, i)\n",
    "                print(\"SOM training epoches %d\" % i)\n",
    "                print(\"neighborhood radius \", radius)\n",
    "                # print(self.net)\n",
    "                print(\"-------------------------------------\")\n",
    "            # using spark broadcast to send initial/updated net to all workers\n",
    "            broadcast_net = sc.broadcast(self.net)\n",
    "            # a wrapper function to prevent serialization of whole class\n",
    "            def train_partition_wrapper(x_size, y_size, num_features):\n",
    "\n",
    "                def train_partition(partition_rows):\n",
    "                    # getting broadcasted net w_{k}(t_0) in each worker \n",
    "                    partition_net = broadcast_net.value\n",
    "                    # Initializing numerator and denominator of Eq (1) to zero to accumulate local sum\n",
    "                    part_sum_numerator = np.array(np.zeros([x_size, y_size, num_features]))\n",
    "                    part_sum_denominator = np.array(np.zeros([x_size, y_size, 1]))\n",
    "\n",
    "                    for row_t in partition_rows:\n",
    "                        # find bmu using utility function\n",
    "                        bmu, bmu_idx = find_bmu(row_t['features'], partition_net)\n",
    "                        \n",
    "                        for x in range(x_size):\n",
    "                            for y in range(y_size):\n",
    "                                # find distance between each neuron and BMU\n",
    "                                w_dist = np.sum((np.array([x, y]) - bmu_idx) ** 2)\n",
    "                                # if the distance is within the current neighbourhood radius\n",
    "                                if w_dist <= radius ** 2:\n",
    "                                    # calculate influence using eq (4)\n",
    "                                    influence = calculate_influence(w_dist, radius)\n",
    "                                    # accumulate local sum for denominator and numerator\n",
    "                                    part_sum_denominator[x, y, :] = part_sum_denominator[x, y, :] + influence\n",
    "                                    new_w = influence * row_t['features']\n",
    "                                    part_sum_numerator[x, y, :] = part_sum_numerator[x, y, :] + new_w\n",
    "                    yield Row(num=part_sum_numerator, den=part_sum_denominator)\n",
    "\n",
    "                return train_partition\n",
    "            # initialize numerator and denominator for global sum to zero\n",
    "            epoch_sum_num = np.array(np.zeros([self.network_dimensions[0], self.network_dimensions[1], self.num_features]))\n",
    "            epoch_sum_den = np.array(np.zeros([self.network_dimensions[0], self.network_dimensions[1], self.num_features]))\n",
    "            part_sum_rdd = rdd.mapPartitions(train_partition_wrapper(self.network_dimensions[0], self.network_dimensions[1], self.num_features))\n",
    "            # combine local sum by each task into global sum\n",
    "            for row in part_sum_rdd.collect():\n",
    "                epoch_sum_num += row['num']\n",
    "                epoch_sum_den += row['den']\n",
    "            # update the net\n",
    "            self.net = epoch_sum_num/epoch_sum_den\n",
    "        # visualization\n",
    "        if fig is not None:\n",
    "            plt.show()\n",
    "\n",
    "def decay_radius(self, iteration):\n",
    "        return self.init_radius * np.exp(-iteration / self.time_constant)\n",
    "\n",
    "def show_plot(self, fig, position, epoch):\n",
    "        # setup axes\n",
    "        ax = fig.add_subplot(2, 5, position, aspect=\"equal\")\n",
    "        ax.set_xlim((0, self.net.shape[0] + 1))\n",
    "        ax.set_ylim((0, self.net.shape[1] + 1))\n",
    "        ax.set_title('Ep: %d' % epoch)\n",
    "\n",
    "        # plot the rectangles\n",
    "        for x in range(1, self.net.shape[0] + 1):\n",
    "            for y in range(1, self.net.shape[1] + 1):\n",
    "                ax.add_patch(patches.Rectangle((x - 0.5, y - 0.5), 1, 1,\n",
    "                                               facecolor=self.net[x - 1, y - 1, :],\n",
    "                                               edgecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of getting prediction (BMU and its coordinate) for each feature is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, df):\n",
    "        # find its Best Matching Unit\n",
    "        column_names = df.columns\n",
    "\n",
    "        def prediction_wrapper(net):\n",
    "            def prediction_map_func(row):\n",
    "                cols_map = {}\n",
    "                for col in column_names:\n",
    "                    cols_map[col] = row[col]\n",
    "                bmu, bmu_idx = find_bmu(row['features'], net)\n",
    "                cols_map[\"bmu\"] = Vectors.dense(bmu[0])\n",
    "                cols_map[\"bmu_idx\"] = Vectors.dense(bmu_idx)\n",
    "                return Row(**cols_map)\n",
    "            rdd_prediction = df.rdd.map(lambda row: prediction_map_func(row))\n",
    "            # getting existing sparkContext\n",
    "            sc = SparkContext.getOrCreate()\n",
    "            sqlContext = SQLContext(sc)\n",
    "            return sqlContext.createDataFrame(rdd_prediction)\n",
    "        return prediction_wrapper(self.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "# create entry points to spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+------+----------+-----+\n",
      "|Year AD|Year BS|   Crop|  Area|Production|Yield|\n",
      "+-------+-------+-------+------+----------+-----+\n",
      "|1984/85|2041/42|OILSEED|127820|     84030|  657|\n",
      "|1985/86|2042/43|OILSEED|137920|     78390|  568|\n",
      "|1986/87|2043/44|OILSEED|142890|     82500|  577|\n",
      "|1987/88|2044/45|OILSEED|151490|     94370|  623|\n",
      "|1988/89|2045/46|OILSEED|154860|     99190|  641|\n",
      "|1989/90|2046/47|OILSEED|153660|     98060|  638|\n",
      "|1990/91|2047/48|OILSEED|156310|     92140|  589|\n",
      "|1991/92|2048/49|OILSEED|154570|     87840|  568|\n",
      "|1992/93|2049/50|OILSEED|165240|     93690|  567|\n",
      "|1993/94|2050/51|OILSEED|177486|    107535|  606|\n",
      "+-------+-------+-------+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read data\n",
    "input_data = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "                .option(\"delimiter\", \",\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(\"cash-crops-nepal.csv\")\n",
    "\n",
    "input_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------+----------+-----+\n",
      "|  Year AD|Year BS|     Crop|  Area|Production|Yield|\n",
      "+---------+-------+---------+------+----------+-----+\n",
      "|  1993/94|2050/51|SUGARCANE| 37785|   1293092|34222|\n",
      "|  1997/98|2054/55|     JUTE| 12265|     15545| 1267|\n",
      "|  1994/95|2051/52|   POTATO| 97634|    838932| 8593|\n",
      "|  1988/89|2045/46|  TOBACCO|  7300|      5380|  737|\n",
      "|  1986/87|2043/44|  OILSEED|142890|     82500|  577|\n",
      "|2002/2003|2059/60|  OILSEED|186720|    124931|  669|\n",
      "|  1992/93|2049/50|  OILSEED|165240|     93690|  567|\n",
      "|  1993/94|2050/51|  OILSEED|177486|    107535|  606|\n",
      "|1999/2000|2056/57|  OILSEED|189629|    122751|  647|\n",
      "|  1998/99|2055/56|  TOBACCO|  4422|      3911|  884|\n",
      "+---------+-------+---------+------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# shuffling data and repartitioning to manage level of parallelism\n",
    "input_data = input_data.repartition(8)\n",
    "input_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "|  Year AD|Year BS|     Crop|  Area|Production|Yield|        raw_features|\n",
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "|  1993/94|2050/51|SUGARCANE| 37785|   1293092|34222|[37785.0,1293092....|\n",
      "|  1997/98|2054/55|     JUTE| 12265|     15545| 1267|[12265.0,15545.0,...|\n",
      "|  1994/95|2051/52|   POTATO| 97634|    838932| 8593|[97634.0,838932.0...|\n",
      "|  1988/89|2045/46|  TOBACCO|  7300|      5380|  737|[7300.0,5380.0,73...|\n",
      "|  1986/87|2043/44|  OILSEED|142890|     82500|  577|[142890.0,82500.0...|\n",
      "|2002/2003|2059/60|  OILSEED|186720|    124931|  669|[186720.0,124931....|\n",
      "|  1992/93|2049/50|  OILSEED|165240|     93690|  567|[165240.0,93690.0...|\n",
      "|  1993/94|2050/51|  OILSEED|177486|    107535|  606|[177486.0,107535....|\n",
      "|1999/2000|2056/57|  OILSEED|189629|    122751|  647|[189629.0,122751....|\n",
      "|  1998/99|2055/56|  TOBACCO|  4422|      3911|  884|[4422.0,3911.0,88...|\n",
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting features from dataframe\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [\"Area\", \"Production\", \"Yield\"],\n",
    "    outputCol = \"raw_features\"\n",
    ")\n",
    "input_features = assembler.transform(input_data)\n",
    "input_features.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxAbsScaler from Sparkâ€™s machine learning library is used to normalize the features between 0 to 1 while preserving the original distance as we are later using Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "|  Year AD|Year BS|     Crop|  Area|Production|Yield|            features|\n",
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "|  1993/94|2050/51|SUGARCANE| 37785|   1293092|34222|[0.19842040865624...|\n",
      "|  1997/98|2054/55|     JUTE| 12265|     15545| 1267|[0.06440720688550...|\n",
      "|  1994/95|2051/52|   POTATO| 97634|    838932| 8593|[0.51270552279327...|\n",
      "|  1988/89|2045/46|  TOBACCO|  7300|      5380|  737|[0.03833449737172...|\n",
      "|  1986/87|2043/44|  OILSEED|142890|     82500|  577|[0.75035840129392...|\n",
      "|2002/2003|2059/60|  OILSEED|186720|    124931|  669|[0.98052292455455...|\n",
      "|  1992/93|2049/50|  OILSEED|165240|     93690|  567|[0.86772497886351...|\n",
      "|  1993/94|2050/51|  OILSEED|177486|    107535|  606|[0.93203241102983...|\n",
      "|1999/2000|2056/57|  OILSEED|189629|    122751|  647|[0.99579895919213...|\n",
      "|  1998/99|2055/56|  TOBACCO|  4422|      3911|  884|[0.02322125306544...|\n",
      "+---------+-------+---------+------+----------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MaxAbsScaler is used to preserve the original features and distance as we will later use Euclidean distance\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "scaler = MaxAbsScaler(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "scalerModel = scaler.fit(input_features)\n",
    "scaledData = scalerModel.transform(input_features).drop(\"raw_features\")\n",
    "scaledData.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFUAAABpCAYAAABLTW+MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAF1klEQVR4nO2dbYgVVRzGf0++ZKwamFuWmmYYZh962UVJLSwJzFKTDCyKJExM7IWSsKIkKfJL4QeDWCoIixJLxUyJwvxgkLVrbqSmaChphS+QuhbZ0r8PM9p1270zO87/emc7PxiYOzPnnOc+98zMOeeeF5kZgXw571wL6IoEUx0IpjoQTHUgmOpAMNWBYKoDVWWqpL2S/pDUUrItzSnufpJWSTohaZ+k+/KItz26e0V8Fkw2s88d4n0dOAlcAlwHfCKp2cy25Z1QVeXUckiaKelLSUslHZX0g6QJKcPWAHcDz5tZi5ltAtYAD3hoLYypMaOBPUB/YCGwUlI/AEkLJK3tINxVQKuZ7So51gxc4yGyGk1dLem3ku3hknMHgSVm9peZLQd2AncAmNliM7uzgzh7A8faHDsK9MlbPFTnM/WuMs/UA3ZmC9A+4LIUcbYAfdsc6wscz6AvkWrMqeUYKEklny8Hfk4RbhfQXdLwkmPXArm/pKB4pl4MPCaph6R7gKuBdUmBzOwEsBJYJKlG0lhgKrDMQ2Q1mvpxm3LqqpJzm4HhwGHgZWC6mR0BkPSspPVl4p0LXED0XH4feMSjOAWgojRSS5oJzDKzcedaSxLVmFMLT2pTJXWT9G2ZsmAgJvXtL+lJoB7oW6Y8GCBlTpU0iKiQ/aavnK5B2tt/CfA08LeflK5DYo1K0p3AQTNrkjS+zHWzgdkANTU1dSNGjMhLY1XS1NR02Mxq2z1pZmU34BVgP7AX+BX4HXi3XJi6ujrr6gCN1sH3T7z9zewZMxtkZkOBGcAGM7s/l5+7ixLKqQ50qpXKzDYCG12UdCFCTnUgmOpAxRuppzUmttS1y8K1vTKneVPvbI1Rx+c/milcyKkOBFMdCKY6EEx1IJjqQDDVgWCqA8FUB4KpDgRTHQimOhBMdSCY6kAw1YGKN/3dXHtbpnDLHnopc5oHB0zOHDYLIac6EEx1INFUSYMlfSFpu6Rtkh6vhLAik+aZ2go8ZWZbJPUBmiR9ZmbbnbUVljSdKX4xsy3x/nFgBzDQW1iR6dQzVdJQ4HqibuKBDuhMp9/ewEfAE2bWdkwSkmZLapTUeOjQoTw1Fo60/VN7EBn6npmtbO8aM2sws3ozq6+tbb8z3P+FNG9/AW8BO8zsNX9JxSdNTh1LNDD2Vklb422Ss65Ck1iksmjEsZKuC/xLqFE5EEx1oOKtVKNX3JIp3Kxpr2ZO88UNUzKFWzwxzVji/xJyqgPBVAeCqQ4EUx0IpjoQTHUgmOpAMNWBYKoDwVQHgqkOBFMdCKY6EEx1oOJNf2Pmb6p0kiy+MlsTXlZCTnUgmOpA2v/9J0raKWm3pAXeoopOmv/9uxFNkn07MBK4V9JIb2FFJk1OHQXsNrMfzewk8AHR3KOBDkhj6kDgp5LP+wm9/sqSW5GqdAY14E9J3+cVdw70J5rINk+GdHQijakHgMElnwfFx87AzBqABgBJjWZW30mRblRaT5rb/xtguKQrJPUkmkVtja+sYpOmL1WrpHnAp0A34G1zmsu5q5DqmWpm60gxS3kJDdnkuFFRPYWZ6LtIhGqqA5lNTaq6Sjpf0vL4/OZ4EIYLacZ6SRofrw50quPyC156EielbW8jemHtAYYBPYlWzBnZ5pq5wBvx/gxgeZa0Uuq5FLgh3u9DtKxHWz3jgbVeGjo1KW0HpKm6TgXeifc/BCa0WfckN6ptrFdWU9NUXU9fY2atREsVXZQxvdQkjPW6UVKzpPWSXNaigupcOikzCWO9tgBDzKwlHgiymmgdltzJmlPTVF1PXyOpO3AhcCRjeokkjfUys2Nm1hLvrwN6SOrvoSWrqWmqrmuAB+P96UQThLsUitOM9ZI04NQzXdIoou/u8yOfxRt3EtFbdg/wXHxsETAl3u8FrAB2A18Dwxzf/uMAA74DtsbbJGAOMCe+Zh7Rol7NwFfAGC89oUblQKhRORBMdSCY6kAw1YFgqgPBVAeCqQ4EUx34BwXRRLvZ3WBjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from our_som2 import SOM\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "som = SOM(3,3,3)\n",
    "init_fig = plt.figure()\n",
    "som.show_plot(init_fig, 1, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 20.0 failed 1 times, most recent failure: Lost task 3.0 in stage 20.0 (TID 23) (staff-3266.wireless.ua.pt executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-24-b7a04c7906b4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m## no learning rate needed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0msom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscaledData\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m200\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/OneDrive - Universidade de Aveiro/ROOT/WORK_UA/K02_Courses/Current-Courses/K02E39_TMBD/shared/session12/notebooks/our_som2.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, df, num_epochs, resetWeights)\u001B[0m\n\u001B[1;32m    136\u001B[0m             \u001B[0mepoch_sum_den\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork_dimensions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork_dimensions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_features\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m             \u001B[0mpart_sum_rdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_partition_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork_dimensions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnetwork_dimensions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_features\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpart_sum_rdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    139\u001B[0m                 \u001B[0mepoch_sum_num\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'num'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m                 \u001B[0mepoch_sum_den\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'den'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    948\u001B[0m         \"\"\"\n\u001B[1;32m    949\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 950\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    951\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    952\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1309\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1310\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 20.0 failed 1 times, most recent failure: Lost task 3.0 in stage 20.0 (TID 23) (staff-3266.wireless.ua.pt executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## no learning rate needed\n",
    "som.train(scaledData, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output prompt above shows the expected nature of neighborhood radius decay. Also, the visualization shows that similar colors are neighbors and clusters are developed similar to previous post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 28) (staff-3266.wireless.ua.pt executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-b81c0b197916>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprediction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msom\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mscaledData\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprediction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive - Universidade de Aveiro/ROOT/WORK_UA/K02_Courses/Current-Courses/K02E39_TMBD/shared/session12/notebooks/our_som2.py\u001B[0m in \u001B[0;36mpredict\u001B[0;34m(self, df)\u001B[0m\n\u001B[1;32m    162\u001B[0m             \u001B[0msqlContext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSQLContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msqlContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd_prediction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 164\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mprediction_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/OneDrive - Universidade de Aveiro/ROOT/WORK_UA/K02_Courses/Current-Courses/K02E39_TMBD/shared/session12/notebooks/our_som2.py\u001B[0m in \u001B[0;36mprediction_wrapper\u001B[0;34m(net)\u001B[0m\n\u001B[1;32m    161\u001B[0m             \u001B[0msc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetOrCreate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    162\u001B[0m             \u001B[0msqlContext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mSQLContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 163\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0msqlContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd_prediction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    164\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mprediction_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnet\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/context.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    371\u001B[0m         \u001B[0mPy4JJavaError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m...\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    372\u001B[0m         \"\"\"\n\u001B[0;32m--> 373\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    374\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregisterDataFrameAsTable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtableName\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36mcreateDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    673\u001B[0m             return super(SparkSession, self).createDataFrame(\n\u001B[1;32m    674\u001B[0m                 data, schema, samplingRatio, verifySchema)\n\u001B[0;32m--> 675\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    676\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_create_dataframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverifySchema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m    696\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    697\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRDD\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 698\u001B[0;31m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromRDD\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    699\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    700\u001B[0m             \u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_createFromLocal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprepare\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_createFromRDD\u001B[0;34m(self, rdd, schema, samplingRatio)\u001B[0m\n\u001B[1;32m    484\u001B[0m         \"\"\"\n\u001B[1;32m    485\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mschema\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 486\u001B[0;31m             \u001B[0mstruct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inferSchema\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msamplingRatio\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnames\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mschema\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    487\u001B[0m             \u001B[0mconverter\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_converter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstruct\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    488\u001B[0m             \u001B[0mrdd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconverter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/session.py\u001B[0m in \u001B[0;36m_inferSchema\u001B[0;34m(self, rdd, samplingRatio, names)\u001B[0m\n\u001B[1;32m    458\u001B[0m         \u001B[0;34m:\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mpyspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtypes\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStructType\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m         \"\"\"\n\u001B[0;32m--> 460\u001B[0;31m         \u001B[0mfirst\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfirst\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    461\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mfirst\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    462\u001B[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mfirst\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1586\u001B[0m         \u001B[0mValueError\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mRDD\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mempty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1587\u001B[0m         \"\"\"\n\u001B[0;32m-> 1588\u001B[0;31m         \u001B[0mrs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1589\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1590\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/rdd.py\u001B[0m in \u001B[0;36mtake\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m   1566\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1567\u001B[0m             \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartsScanned\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mnumPartsToTry\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtotalParts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1568\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtakeUpToNumLeft\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1569\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1570\u001B[0m             \u001B[0mitems\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/context.py\u001B[0m in \u001B[0;36mrunJob\u001B[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001B[0m\n\u001B[1;32m   1225\u001B[0m         \u001B[0;31m# SparkContext#runJob.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1226\u001B[0m         \u001B[0mmappedRDD\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionFunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1227\u001B[0;31m         \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunJob\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpartitions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1228\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmappedRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1229\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1307\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1308\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1309\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1310\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1311\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    109\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 111\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    112\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    113\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 28) (staff-3266.wireless.ua.pt executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/eugenio/Library/Python/3.8/lib/python/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 481, in main\n    raise RuntimeError((\"Python in worker has different version %s than that in \" +\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:545)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:703)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:685)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:498)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "prediction = som.predict(scaledData)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: It is counter intuitive in Big Data ecosystem to use dataframe.collect() as all data gets into the driver node but I have used it for simplicity of visualization (for small dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "fig = plt.figure()\n",
    "# setup axes\n",
    "ax = fig.add_subplot(111)\n",
    "scale = 50\n",
    "ax.set_xlim((0, som.net.shape[0]*scale))\n",
    "ax.set_ylim((0, som.net.shape[1]*scale))\n",
    "ax.set_title(\"Cash Crops Clustering by using SOM\")\n",
    "\n",
    "for x in range(0, som.net.shape[0]):\n",
    "    for y in range(0, som.net.shape[1]):\n",
    "        ax.add_patch(patches.Rectangle((x*scale, y*scale), scale, scale,\n",
    "                     facecolor='white',\n",
    "                     edgecolor='grey'))\n",
    "legend_map = {}\n",
    "        \n",
    "for row in prediction.collect():\n",
    "    x_cor = row['bmu_idx'][0] * scale\n",
    "    y_cor = row['bmu_idx'][1] * scale\n",
    "    x_cor = np.random.randint(x_cor*1.05, (x_cor + scale*0.8))\n",
    "    y_cor = np.random.randint(y_cor*1.05, (y_cor + scale*0.8))\n",
    "    color = row['bmu']\n",
    "    marker = \"$\\\\ \" + row['Crop'][0]+\"$\"\n",
    "    marker = marker.lower()\n",
    "    ax.plot(x_cor, y_cor, color=color, marker=marker, markersize=10)\n",
    "    label = row['Crop']\n",
    "    if not label in legend_map:\n",
    "        legend_map[label] =  mlines.Line2D([], [], color='black', marker=marker, linestyle='None',\n",
    "                          markersize=10, label=label)\n",
    "plt.legend(handles=list(legend_map.values()), bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the visualization above that the data has been clustered quite nicely. Moreover, the clustering is similar to SOM1.ipynb. So, we have verified our Apache Spark Implementation of SOM Batch Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}